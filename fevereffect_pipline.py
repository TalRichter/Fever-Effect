{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQBicAfirLuCd7xyY0pJzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalRichter/Fever-Effect/blob/main/fevereffect_pipline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74xKrwgCpO99"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LassoCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, f1_score, accuracy_score, precision_score, recall_score,\n",
        "    roc_curve, auc, balanced_accuracy_score, average_precision_score, matthews_corrcoef)\n",
        "import shap\n",
        "from sklearn.utils import resample\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "\n",
        "\n",
        "# Function Definitions\n",
        "def load_data(gene_expression_file, label_file):\n",
        "    \"\"\"Load gene expression and label data.\"\"\"\n",
        "    gene_expression = pd.read_csv(gene_expression_file, index_col=0, sep=\"\\t\")\n",
        "    label_group_all = pd.read_csv(label_file, index_col=0)\n",
        "    return gene_expression, label_group_all\n",
        "\n",
        "def preprocess_data(gene_expression, label_group_all):\n",
        "    \"\"\"Preprocess data by filtering, encoding, and removing rows with missing values.\"\"\"\n",
        "    label_group_all['cluster_verbal_iq'] = label_group_all['verbal_iq'].apply(lambda x: 'high' if x >= 70 else 'low')\n",
        "    ind_info = label_group_all[['sex', 'race', 'any_gastro_disorders_proband',\n",
        "                                'any_neurological_disorders_proband',\n",
        "                                'any_neurological_disorders_in_family',\n",
        "                                'any_autoimmune_disease_proband',\n",
        "                                'any_autoimmune_disease_in_family',\n",
        "                                'any_chronic_illnesses_in_family',\n",
        "                                'any_maternal_infections_in_preg',\n",
        "                                'any_autoimmune_disease_mat',\n",
        "                                'any_language_disorders_in_family',\n",
        "                                'cluster_verbal_iq', 'nonverbal_iq', 'verbal_iq']]\n",
        "    ind_info_cleaned = ind_info.dropna()\n",
        "    row_names = ind_info_cleaned.index\n",
        "    gene_expression_cleaned = gene_expression.loc[row_names].T\n",
        "    return gene_expression_cleaned, ind_info_cleaned\n",
        "\n",
        "def convert_target_to_binary(label_group_all, target_column='Target'):\n",
        "    \"\"\"Convert the target column to a binary format.\"\"\"\n",
        "    if target_column not in label_group_all.columns:\n",
        "        raise KeyError(f\"Column '{target_column}' not found in the dataframe.\")\n",
        "    Target = label_group_all.loc[:, target_column]\n",
        "    Target = pd.DataFrame(Target.replace(to_replace=['no', 'yes'], value=[0, 1]))\n",
        "    Target_np = np.array(Target)\n",
        "    return Target, Target_np\n",
        "\n",
        "def feature_engineering(gene_expression, ind_info):\n",
        "    \"\"\"Feature engineering and selection.\"\"\"\n",
        "    X_encoded = pd.get_dummies(ind_info)\n",
        "    gene_expression = variance_threshold_selector(gene_expression.T, threshold=0.1)\n",
        "    X_encoded = X_encoded.reindex(gene_expression.index)\n",
        "    combined = gene_expression.merge(X_encoded, left_index=True, right_index=True)\n",
        "    return combined\n",
        "\n",
        "def variance_threshold_selector(data, threshold=0.2):\n",
        "    \"\"\"Remove features with low variance.\"\"\"\n",
        "    selector = VarianceThreshold(threshold)\n",
        "    selector.fit(data)\n",
        "    return data[data.columns[selector.get_support(indices=True)]]\n",
        "\n",
        "def train_test_split_and_scale(combined, target):\n",
        "    \"\"\"Split the data into training and testing sets and scale features.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(combined, target, test_size=0.2, random_state=0)\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "def kmeans_clustering(X_train, X_test, n_clusters=30, n_init=10):\n",
        "    \"\"\"Apply KMeans clustering to the dataset.\"\"\"\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init)\n",
        "    X_train['Cluster'] = kmeans.fit_predict(X_train)\n",
        "    X_test['Cluster'] = kmeans.predict(X_test)\n",
        "    return X_train, X_test\n",
        "\n",
        "def feature_selection(X_train, y_train, n_features):\n",
        "    \"\"\"Select important features using Lasso.\"\"\"\n",
        "    lasso = LassoCV(cv=5).fit(X_train, y_train)\n",
        "    important_genes = np.abs(lasso.coef_).argsort()[-n_features:][::-1]\n",
        "    return X_train.iloc[:, important_genes], important_genes\n",
        "\n",
        "\n",
        "def train_xgboost(X_train, y_train, params, cv_method='grid', cv_folds=5, n_jobs=10, n_iter=50):\n",
        "    \"\"\"Train an XGBoost model using specified cross-validation method.\"\"\"\n",
        "    model = XGBClassifier(objective='binary:logistic', seed=1, colsample_bytree=0.5, use_label_encoder=False)\n",
        "    if cv_method == 'grid':\n",
        "        optimal_params = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=params,\n",
        "            scoring='roc_auc',\n",
        "            verbose=0,\n",
        "            n_jobs=n_jobs,\n",
        "            cv=cv_folds\n",
        "        )\n",
        "    elif cv_method == 'random':\n",
        "        optimal_params = RandomizedSearchCV(\n",
        "            estimator=model,\n",
        "            param_distributions=params,\n",
        "            scoring='roc_auc',\n",
        "            verbose=0,\n",
        "            n_jobs=n_jobs,\n",
        "            cv=cv_folds,\n",
        "            n_iter=n_iter\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"cv_method must be either 'grid' or 'random'\")\n",
        "\n",
        "    optimal_params.fit(X_train, y_train)\n",
        "    return optimal_params\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, plot=False):\n",
        "    \"\"\"Evaluate the performance of the model.\"\"\"\n",
        "    y_probs = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = model.predict(X_test)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    balanced_acc = balanced_accuracy_score(y_test, y_pred, adjusted=True)\n",
        "    average_precision = average_precision_score(y_test, y_probs)\n",
        "    if plot:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "    return {\n",
        "        'F1 Score': f1,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'AUC-ROC': roc_auc,\n",
        "    }\n",
        "\n",
        "def multiple_downsampling(data, target_column, iterations=5):\n",
        "    \"\"\"Perform multiple downsampling iterations of the majority class.\"\"\"\n",
        "    downsampled_datasets = []\n",
        "    majority_class = data[data[target_column] == 0]\n",
        "    minority_class = data[data[target_column] == 1]\n",
        "    for i in range(iterations):\n",
        "        majority_downsampled = resample(majority_class,\n",
        "                                        replace=False,\n",
        "                                        n_samples=len(minority_class),\n",
        "                                        random_state=i)\n",
        "        downsampled_data = pd.concat([majority_downsampled, minority_class])\n",
        "        downsampled_datasets.append(downsampled_data)\n",
        "    return downsampled_datasets\n",
        "\n",
        "def parse_feature_selection_options(option_str):\n",
        "    # Convert a comma-separated string to a list of integers\n",
        "    return [int(x.strip()) for x in option_str.split(',')]\n",
        "\n",
        "\n",
        "\n",
        "# Main Script\n",
        "if __name__ == \"__main__\":\n",
        "    print(sys.argv)\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Process model parameters.')\n",
        "\n",
        "    # Define arguments\n",
        "    parser.add_argument('-FS', '--feature_selection', default='10,20', help='Feature selection options (comma-separated)')\n",
        "    parser.add_argument('-DS', '--downsampling', type=int, default=5, help='Number of downsampling iterations')\n",
        "    parser.add_argument('-TF', '--top_features', type=int, default=10, help='Number of top features')\n",
        "    parser.add_argument('-OD', '--output_directory', default='/gpfs0/alal/users/richtert/XGBoost_Fever_effect/output/', help='Directory for saving models')\n",
        "    parser.add_argument('-OF', '--output_file', default='/gpfs0/alal/users/richtert/XGBoost_Fever_effect/summary/output.csv', help='File for saving evaluations')\n",
        "    parser.add_argument('-GE', '--gene_expression_file', default=\"/gpfs0/alal/users/richtert/XGBoost_Fever_effect/files/symbol_gi_log.txt\", help='Gene expression data file')\n",
        "    parser.add_argument('-LF', '--label_file', default=\"/gpfs0/alal/users/richtert/XGBoost_Fever_effect/files/sampleTable_GI.csv\", help='Label file')\n",
        "    parser.add_argument('-CVM', '--cv_method', default='grid', choices=['grid', 'random'], help='Cross-validation method: grid or random')\n",
        "\n",
        "\n",
        "    # Parse arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Use arguments\n",
        "    feature_selection_options = parse_feature_selection_options(args.feature_selection)\n",
        "    n_downsampling = args.downsampling\n",
        "    top_features = args.top_features\n",
        "    output_file = args.output_file\n",
        "    gene_expression_file = args.gene_expression_file\n",
        "    label_file = args.label_file\n",
        "    cv_method = args.cv_method\n",
        "    output_directory = args.output_directory\n",
        "\n",
        "    # Directories for storing models, parameters, and results\n",
        "    models_directory = os.path.join(args.output_directory, 'models')\n",
        "    params_directory = os.path.join(args.output_directory, 'params')\n",
        "    results_directory = os.path.join(args.output_directory, 'results')\n",
        "    shap_directory = os.path.join(args.output_directory, 'shap_results')\n",
        "\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    for directory in [models_directory, params_directory, results_directory, shap_directory]:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "\n",
        "    # Load data\n",
        "    gene_expression, label_group_all = load_data(gene_expression_file, label_file)\n",
        "    Target, Target_np = convert_target_to_binary(label_group_all)\n",
        "    gene_expression, ind_info = preprocess_data(gene_expression, label_group_all)\n",
        "    combined = feature_engineering(gene_expression, ind_info)\n",
        "    Target = Target.reindex(combined.index)\n",
        "    combined_with_target = combined.join(Target)\n",
        "\n",
        "    # Define the hyperparameter grid for XGBoost\n",
        "    params = {\n",
        "        'learning_rate': [0.03, 0.04, 0.05],\n",
        "        'max_depth': [8, 9, 10],\n",
        "        'colsample_bylevel': [0.4, 0.5, 0.6],\n",
        "        'lambda': [1, 2 ,3],\n",
        "        'min_child_weight': [1, 3],\n",
        "        'gamma': [0, 0.2, 0.5],\n",
        "        'subsample': [ 0.7, 0.8, 1],\n",
        "        'reg_alpha': [0, 0.5, 1],\n",
        "    }\n",
        "\n",
        "    # Feature selection options and results storage\n",
        "    all_results = []  # To store aggregated results\n",
        "    models = {}\n",
        "    shap_results = {}\n",
        "    SHAP_mean_and_corr = {}\n",
        "\n",
        "    # Multiple downsampling\n",
        "    downsampled_datasets = multiple_downsampling(combined_with_target, 'Target', iterations=n_downsampling)\n",
        "\n",
        "    for n_features in feature_selection_options:\n",
        "        for i, dataset in enumerate(downsampled_datasets):\n",
        "            X_downsampled = dataset.drop('Target', axis=1)\n",
        "            y_downsampled = dataset['Target']\n",
        "            X_train, X_test, y_train, y_test = train_test_split_and_scale(X_downsampled, y_downsampled)\n",
        "            X_train, X_test = kmeans_clustering(X_train, X_test)\n",
        "\n",
        "            # Feature selection\n",
        "            X_train_fs, important_genes = feature_selection(X_train, y_train, n_features)\n",
        "            X_test_fs = X_test.iloc[:, important_genes]\n",
        "\n",
        "            # Cross-validation setup\n",
        "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "            fold_results = []\n",
        "\n",
        "            for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X_train_fs, y_train)):\n",
        "                X_train_fold, X_test_fold = X_train_fs.iloc[train_idx], X_train_fs.iloc[test_idx]\n",
        "                y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
        "\n",
        "                # Train the model\n",
        "                grid_search = train_xgboost(X_train_fold, y_train_fold, params, cv_method=cv_method)\n",
        "                model = grid_search.best_estimator_  # Extract the best estimator\n",
        "\n",
        "\n",
        "                # Save the model in the specified directory\n",
        "                model_file_path = os.path.join(models_directory, f'model_{n_features}_{i}_{fold_idx}.json')\n",
        "                model.save_model(model_file_path)  # Save the model\n",
        "\n",
        "\n",
        "                # Get the model parameters\n",
        "                model_params = model.get_xgb_params()\n",
        "                model_params.update({'n_features': n_features, 'iteration': i, 'fold_idx': fold_idx})\n",
        "\n",
        "\n",
        "                # Save the model parameters in the specified directory\n",
        "                params_file_path = os.path.join(params_directory, f'model_params_{n_features}_{i}_{fold_idx}.json')\n",
        "                with open(params_file_path, 'w') as f:\n",
        "                    json.dump(model_params, f)\n",
        "\n",
        "                # # Store model for later retrieval\n",
        "                # models[(n_features, i, fold_idx)] = model\n",
        "\n",
        "                # Evaluate model\n",
        "                results = evaluate_model(model, X_test_fold, y_test_fold)\n",
        "                results.update({'Num_Features': n_features, 'Subsample_Iteration': i+1, 'Fold_Index': fold_idx})\n",
        "                fold_results.append(results)\n",
        "\n",
        "                # SHAP values\n",
        "                explainer = shap.Explainer(model)\n",
        "                shap_values = explainer(X_test_fold).values\n",
        "\n",
        "                # Convert SHAP values to DataFrame and store\n",
        "                shap_df = pd.DataFrame(shap_values, columns=X_test_fold.columns)\n",
        "                shap_results[(n_features, i, fold_idx)] = shap_df\n",
        "                shap_df = pd.DataFrame(shap_values, columns=X_test_fold.columns)\n",
        "\n",
        "                # Save SHAP values to file\n",
        "                shap_file_path = os.path.join(shap_directory, f'shap_values_{n_features}_{i}_{fold_idx}.csv')\n",
        "                shap_df.to_csv(shap_file_path, index=False)\n",
        "\n",
        "\n",
        "                # Compute the mean absolute SHAP value for each feature\n",
        "                mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "                shap_summary = pd.DataFrame({\n",
        "                    'Feature': X_train_fold.columns,\n",
        "                    'Mean Abs SHAP Value': mean_abs_shap\n",
        "                })\n",
        "\n",
        "                # Correlation\n",
        "                feature_values = X_test_fold.reset_index(drop=True)  # Reset index to align with shap_values\n",
        "                correlations = pd.Series(index=X_train_fold.columns, dtype=float)\n",
        "                for feature in X_train_fold.columns:\n",
        "                    correlations[feature] = shap_df[feature].corr(feature_values[feature])\n",
        "\n",
        "                # Prepare the final summary DataFrame\n",
        "                final_summary_df = shap_summary\n",
        "                final_summary_df['Correlation with SHAP'] = correlations.values\n",
        "\n",
        "                key = (n_features, i, fold_idx)\n",
        "                SHAP_mean_and_corr[key] = final_summary_df.sort_values(by='Mean Abs SHAP Value', ascending=False)\n",
        "\n",
        "            # Aggregate results for this downsampled dataset\n",
        "            iteration_results = {\n",
        "                'Num_Features': n_features,\n",
        "                'Subsample_Iteration': i+1\n",
        "            }\n",
        "            for metric in ['F1 Score', 'Accuracy', 'Precision', 'Recall', 'AUC-ROC']:\n",
        "                iteration_results[f'{metric}_Mean'] = np.mean([r[metric] for r in fold_results])\n",
        "                iteration_results[f'{metric}_Std'] = np.std([r[metric] for r in fold_results])\n",
        "\n",
        "            # Convert the aggregated results to a DataFrame and save to CSV\n",
        "            all_results.append(iteration_results)\n",
        "\n",
        "    # Calculate the overall average for each metric\n",
        "    overall_averages = {}\n",
        "    metrics = ['F1 Score', 'Accuracy', 'Precision', 'Recall', 'AUC-ROC']\n",
        "    for metric in metrics:\n",
        "        overall_averages[f'{metric}_Overall_Mean'] = np.mean([result[f'{metric}_Mean'] for result in all_results])\n",
        "        overall_averages[f'{metric}_Overall_Std'] = np.mean([result[f'{metric}_Std'] for result in all_results])\n",
        "\n",
        "\n",
        "    # Convert the aggregated results to a DataFrame and save to CSV\n",
        "    summary_results_df = pd.DataFrame(all_results)\n",
        "    summary_results_df.to_csv(os.path.join(results_directory, 'summary_evaluation_results.csv'), index=False)\n",
        "\n",
        "\n",
        "    # Save the overall averages to a separate file\n",
        "    overall_averages_df = pd.DataFrame([overall_averages])\n",
        "    overall_averages_df.to_csv(os.path.join(results_directory, 'overall_averages.csv'), index=False)\n",
        "\n",
        "\n",
        "    # Print the summary\n",
        "    print(\"\\nModel Performance Summary:\")\n",
        "    print(summary_results_df)\n",
        "    print(\"\\nOverall Model Performance Summary:\")\n",
        "    print(overall_averages_df)\n",
        "\n",
        "    # To access a specific model's SHAP values, use the `shap_results` dictionary\n",
        "    # Example: SHAP values for the model trained with 10 features, on 2nd subsample, 3rd fold\n",
        "    # specific_shap_values = shap_results[(10, 2, 3)]\n",
        "\n",
        "    # Aggregate SHAP values and correlations across all models\n",
        "    aggregated_shap_values = {}\n",
        "    aggregated_correlations = {}\n",
        "\n",
        "    # Calculate and append mean and standard deviation for each metric across all folds\n",
        "    for key, shap_df in shap_results.items():\n",
        "        for feature in shap_df.columns:\n",
        "            if feature not in aggregated_shap_values:\n",
        "                aggregated_shap_values[feature] = []\n",
        "                aggregated_correlations[feature] = []\n",
        "            aggregated_shap_values[feature].append(np.abs(shap_df[feature]).mean())\n",
        "            aggregated_correlations[feature].append(SHAP_mean_and_corr[key][SHAP_mean_and_corr[key]['Feature'] == feature]['Correlation with SHAP'].iloc[0])\n",
        "\n",
        "    # Calculate the mean of the absolute SHAP values and the mean correlation for each feature\n",
        "    mean_shap_values = {feature: np.mean(values) for feature, values in aggregated_shap_values.items()}\n",
        "    mean_correlations = {feature: np.mean(values) for feature, values in aggregated_correlations.items()}\n",
        "\n",
        "    # Sort features based on the mean absolute SHAP values\n",
        "    sorted_features = sorted(mean_shap_values, key=mean_shap_values.get, reverse=True)\n",
        "\n",
        "    # Extract the top X features and their directions\n",
        "    top_features = sorted_features[:top_features]\n",
        "    top_features_directions = {feature: 'Positive' if mean_correlations[feature] > 0 else 'Negative' for feature in top_features}\n",
        "\n",
        "    # Display the top features and their directions\n",
        "    print(\"Top Features and Their Directions:\")\n",
        "    for feature in top_features:\n",
        "        print(f\"{feature}: {top_features_directions[feature]}\")\n",
        "\n"
      ]
    }
  ]
}
